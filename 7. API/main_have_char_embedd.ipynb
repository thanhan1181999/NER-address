{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_have_char_embedd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgQllUjD0p87",
        "outputId": "f5337359-04ec-44d3-905d-e70ffb06fd34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMPM2_La0ul8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba89a1f-c7c5-47bb-e126-6d7bebad1ed5"
      },
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install git+https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting git+https://github.com/facebookresearch/fastText.git\n",
            "  Cloning https://github.com/facebookresearch/fastText.git to /tmp/pip-req-build-qdbx_2i7\n",
            "  Running command git clone -q https://github.com/facebookresearch/fastText.git /tmp/pip-req-build-qdbx_2i7\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (56.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3082494 sha256=c92e3d70e496d1acee31a617d0f75e4f94b093531d688aec39e9ffaf091e420d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wawwhpox/wheels/69/f8/19/7f0ab407c078795bc9f86e1f6381349254f86fd7d229902355\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d48fmlv7ek7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9b68a4-5b35-4249-ce4c-9bc1686d631e"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install keras==2.2.5\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 26kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 28.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 28.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=64bfbca752b969739b8c0f2b9e2813e292b9e6f76b0ba452ffceadb467978538\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.5\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-dwm27igh\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-dwm27igh\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=36624049fc3955d57681ecddc6fcdf3cba9e98676055f547c27ebeef19e140c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zlgh8lm1/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYwlQRHnfMg1"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import argmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7mFtI8qfSm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd1212a-4d82-4d50-f712-edcaaa1a7ae6"
      },
      "source": [
        "from keras.models import Sequential, Model, Input\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Activation, Bidirectional, Masking, Embedding, Dropout, Flatten, concatenate, Conv1D, MaxPool1D, Lambda, Softmax\n",
        "from keras import backend as K\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras_contrib.losses import  crf_loss\n",
        "from keras_contrib.metrics import crf_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-XKNXF5AMln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cda3b47-eb14-4c2a-9533-05197c8e7e46"
      },
      "source": [
        "from keras.models import load_model\n",
        "m = load_model(\"/content/drive/MyDrive/model/model-ver1.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHnZKlbNYGSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18da75b5-75fe-4dc5-c4ae-a40c4f8370f0"
      },
      "source": [
        "#get word embedding libary\n",
        "import fasttext.util\n",
        "print('Creating fastText...')\n",
        "word2vec = fasttext.load_model('/content/drive/MyDrive/fast_text/cc.vi.300.bin')\n",
        "print('fastText created!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating fastText...\n",
            "fastText created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKHABcXhSjC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76de295e-88a9-4c15-c087-4cd3af1f2624"
      },
      "source": [
        "!pip install vncorenlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2020.12.5)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=2967bfbbaae8c877642f2fea583f7a2b45e56fad44833dd28674233ae5c35769\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeDy9kM_Sj53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cd6cfe-9a2d-453e-8dbb-1588c58cfdb9"
      },
      "source": [
        "# get word embedding\n",
        "from vncorenlp import VnCoreNLP\n",
        "print('Creating token word...')\n",
        "annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "print('token word created')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating token word...\n",
            "token word created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D674NiI6Syo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e0f612-29e3-4757-beb0-9087bb8b5598"
      },
      "source": [
        "# aphabet tag library\n",
        "tags = {'pad': 0, 'LOCATION_HOMENUMBER': 1, 'LOCATION_STREET': 2, 'LOCATION_WARD': 3, 'LOCATION_DISTRICT': 4, 'LOCATION_PROVINCE': 5, 'LOCATION_COUNTRY': 6, 'LOCATION_POSTCODE': 7, 'LOCATION_SPECIAL': 8, 'LOCATION_NER': 9, 'OBJ': 10, 'OBJ_FEATURE': 11, 'PRE': 12, 'UNKNOW': 13 }\n",
        "aphabet_tag = []\n",
        "for key in tags:\n",
        "  aphabet_tag.append(key)\n",
        "# aphabet_tag = ['pad', 'LOCATION_HOMENUMBER', 'LOCATION_STREET', 'LOCATION_WARD', 'LOCATION_DISTRICT', 'LOCATION_PROVINCE', 'LOCATION_COUNTRY', 'LOCATION_POSTCODE', 'OBJ', 'OBJ_FEATURE', 'PRE', 'UNKNOW', 'LOCATION_SPECIAL']\n",
        "print(aphabet_tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pad', 'LOCATION_HOMENUMBER', 'LOCATION_STREET', 'LOCATION_WARD', 'LOCATION_DISTRICT', 'LOCATION_PROVINCE', 'LOCATION_COUNTRY', 'LOCATION_POSTCODE', 'LOCATION_SPECIAL', 'LOCATION_NER', 'OBJ', 'OBJ_FEATURE', 'PRE', 'UNKNOW']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB5ojXXev9Dk"
      },
      "source": [
        "def map_number_and_punct(word):\n",
        "    # check hem va ngach\n",
        "    num_of_seperate=0\n",
        "    dem=0\n",
        "    for char in word:\n",
        "      if not char.isnumeric() and char!=\"/\":\n",
        "        dem+=1        \n",
        "      if char==\"/\":\n",
        "        num_of_seperate+=1\n",
        "    if dem==0:\n",
        "      if len(word)>1 and num_of_seperate==1:\n",
        "        return u'<ngach>'\n",
        "      if len(word)>1 and num_of_seperate>2:\n",
        "        return u'<hem>'\n",
        "\n",
        "    if re.match(r\"^[0-9]{2}0{3}$\", word):\n",
        "        return u'<postcode>'\n",
        "\n",
        "    if word.isnumeric():\n",
        "        word = u'<number>'\n",
        "    elif word in [u',', u'<', u'.', u'>', u'/', u'?', u'..', u'...', u'....', u':', u';', u'\"', u\"'\", u'[', u'{', u']',\n",
        "                  u'}', u'|', u'\\\\', u'`', u'~', u'!', u'@', u'#', u'$', u'%', u'^', u'&', u'*', u'(', u')', u'-', u'+',\n",
        "                  u'=']:\n",
        "        word = u'<punct>'\n",
        "\n",
        "    word = word.replace(\"_\",\" \")\n",
        "    return word\n",
        "\n",
        "def get_word_embbeding(sen_words,max_length=32,embedd_dim = 300):\n",
        "  X = np.empty([1, max_length, embedd_dim])\n",
        "  unknown_embedd = np.random.uniform(-0.01, 0.01, [1, 300])\n",
        "  length = len(sen_words)\n",
        "  for i in range(length):\n",
        "    try:\n",
        "      vec = word2vec.get_word_vector(map_number_and_punct(sen_words[i])).tolist()\n",
        "    except:\n",
        "      vec = unknown_embedd\n",
        "    X[0, i, :] = vec\n",
        "  X[0,length:] = np.zeros([1, embedd_dim])\n",
        "  return np.array(X)\n",
        "# sen_words_embedd = get_word_embbeding(sen_words[0],max_length=25,embedd_dim = 300)\n",
        "# print(sen_words_embedd.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAVbVKV3ZpgG"
      },
      "source": [
        "#get char embedding\n",
        "def read_char_vocab(file_path):\n",
        "  char_vocab = []\n",
        "  for line in open(file_path, encoding='utf-8'):\n",
        "    char_vocab.append(line.splitlines()[0])      \n",
        "  return char_vocab\n",
        "\n",
        "def char_to_int(char_vocab_data):\n",
        "  dic = {}\n",
        "  index = -1\n",
        "  for char in char_vocab_data:\n",
        "    try:\n",
        "      dic[char]\n",
        "    except:\n",
        "      index = index + 1\n",
        "      dic[char]=index\n",
        "  return dic\n",
        " \n",
        "def get_char_encode(sentence,max_length_of_a_sentence,max_length_of_a_word):\n",
        "  char_vocab_data = read_char_vocab(\"/content/drive/MyDrive/fast_text/VISCII_short.txt\")\n",
        "  dic = char_to_int(char_vocab_data)\n",
        "  # sentence  words is a sentence | ['i','am','an']\n",
        "  sentence_encoded = np.zeros([max_length_of_a_sentence,max_length_of_a_word]) # 25*25\n",
        "  for j in range(len(sentence)):  \n",
        "    word = sentence[j].lower()\n",
        "    # integer_encoded = [char_to_int[char] for char in word]\n",
        "    word_encoded = np.zeros(max_length_of_a_word)\n",
        "    for k in range(len(word)):\n",
        "      char = word[k]\n",
        "      try:\n",
        "        word_encoded[k]= dic[char]\n",
        "      except:\n",
        "        print(\"error : \" + str(char)+\" \"+str(word)+\" \"+str(len(word)))\n",
        "        word_encoded[k]= dic['[unk]']\n",
        "    # sentence encoded\n",
        "    sentence_encoded[j] = word_encoded\n",
        "  return sentence_encoded\n",
        "\n",
        "def get_charEmbedd_form_encode(charEnocde):\n",
        "  LEN_OF_VOCAB = 137\n",
        "  shape = charEnocde.shape\n",
        "  char_embedd = np.zeros([shape[0],shape[1],LEN_OF_VOCAB])\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      char_int = charEnocde[i,j]\n",
        "      char_int = char_int.astype(np.int64)\n",
        "      onehot = np.zeros(LEN_OF_VOCAB)\n",
        "      onehot[char_int] = 1\n",
        "      char_embedd[i,j,:] = onehot\n",
        "  return char_embedd\n",
        "\n",
        "# return onehot_encoded_of_a_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkdZwHxH362H",
        "outputId": "220f31e9-4c94-4b26-a87c-04d315b029d8"
      },
      "source": [
        "!unzip /content/drive/MyDrive/ngrok/ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/ngrok/ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BloS0Pnz4UA5",
        "outputId": "19a121d6-ffad-4994-cc15-dca3ed3e7405"
      },
      "source": [
        "!./ngrok authtoken 1ow8g2aFZ4wILzg2dqHin5bUy5t_34f12x5zn89tGiLLCi4Wy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6haWtvVihD-3",
        "outputId": "02ddba84-d8ea-4ed2-81bd-35c76399af38"
      },
      "source": [
        "# query = \"số nhà 103 - A12\"\n",
        "# # query = \"tìm các nơi tổ chức tang lễ đang mở cửa xung quanh khu này\"\n",
        "# try:\n",
        "#   sen_words = annotator.tokenize(query)[0]\n",
        "#   num_of_word = len(sen_words)\n",
        "# except:\n",
        "#   print('Creating token word...')\n",
        "#   annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "#   print('token word created')\n",
        "#   sen_words = annotator.tokenize(query)[0]\n",
        "#   num_of_word = len(sen_words)\n",
        "# print(sen_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['số', 'nhà', '103', '-', 'A12']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzH_rgZSZDi"
      },
      "source": [
        "# hàm tiền xử lý câu đầu vào\n",
        "# bỏ các dấu đặc biệt : \n",
        "special_char = [\",\",\"_\"]\n",
        "# bỏ các từ đặc biệt: quận, huyện, tỉnh, thành phố, \n",
        "fail_text = [\"ward\", \"xã\", \"phường\", \"thị trấn\", \"quận\", \"huyện\",\"tỉnh\",\"district\",\"thị xã\",\"thành phố\",\"province\"]\n",
        "def process_word(word):\n",
        "  for char in special_char:\n",
        "    if char in word:\n",
        "      word = word.replace(char,\" \")\n",
        "  for text in fail_text:\n",
        "    if text in word:\n",
        "      word = word.replace(text,\" \")\n",
        "  return word.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-opCmTfYVEG",
        "outputId": "b16d1950-4635-4e4f-f944-3f4d59fdd9a2"
      },
      "source": [
        "# query = \"tìm kiếm khách sạn 5 sao xung quanh chỗ này\"\n",
        "# query = \"sân bay đang mở cửa xung quanh 144 lô 14a trung hòa cầu giấy hà nội việt nam\"\n",
        "# query = \" nghĩa đô cầu giấy hà nội việt nam 10000\"\n",
        "# query = \"khách sạn 5 sao gần số 6 ngõ 5 cầu giấy\"\n",
        "# query = \"293 Trần Đại Nghĩa quận Hai Bà Trưng Hà Nội\"\n",
        "# query = \"nguyễn thị định trung hoà cầu giấy hà nội việt nam 10000\"\n",
        "# query = \"144 xuân thủy cầu giấy hà nội\"\n",
        "query = \"tìm các nhà thương tại phở cuốn hương mai vũ phạm hàm\"\n",
        "try:\n",
        "  sen_words = annotator.tokenize(query)[0]\n",
        "  num_of_word = len(sen_words)\n",
        "except:\n",
        "  print('Creating token word...')\n",
        "  annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "  print('token word created')\n",
        "  sen_words = annotator.tokenize(query)[0]\n",
        "  num_of_word = len(sen_words)\n",
        "print(sen_words)\n",
        "\n",
        "for i in range(len(sen_words)):\n",
        "  sen_words[i] = process_word(sen_words[i])\n",
        "\n",
        "# get word embedding\n",
        "sen_words_embedd = get_word_embbeding(sen_words,max_length=42,embedd_dim = 300)\n",
        "    \n",
        "# get char embedding\n",
        "char_encode = get_char_encode(sen_words,42,32)\n",
        "\n",
        "char_embedd = get_charEmbedd_form_encode(char_encode)\n",
        "char_embedd = char_embedd.reshape(1,char_embedd.shape[0], char_embedd.shape[1],char_embedd.shape[2])\n",
        "\n",
        "# predict\n",
        "output = m.predict([char_embedd,sen_words_embedd])\n",
        "\n",
        "# return [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5] or convert to encode tag\n",
        "tag_encode = []\n",
        "for i in range(42): \n",
        "  tag_encode.append(argmax(output[0,i,:]))\n",
        "\n",
        "# return [street, homenumber,.....] or only get [num_of_word] first element=> convert to tag\n",
        "tag_result=[]\n",
        "for i in range(num_of_word):\n",
        "  tag_result.append(aphabet_tag[tag_encode[i]])\n",
        "  arr_tag_result=np.array(tag_result)\n",
        "\n",
        "# return {'nhaf_hàng': OBJ,....} \n",
        "result={}\n",
        "for i in range(num_of_word):\n",
        "  result[sen_words[i]] = str(arr_tag_result[i])\n",
        "\n",
        "print(result)\n",
        "#==========================\n",
        "dic_result = {}\n",
        "for word in result:\n",
        "  tag = result[word]\n",
        "  try:\n",
        "    dic_result[tag] += \" \" + word\n",
        "  except:\n",
        "    dic_result[tag] = word\n",
        "print(dic_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tìm', 'các', 'nhà_thương', 'tại', 'phở', 'cuốn', 'hương', 'mai', 'vũ', 'phạm', 'hàm']\n",
            "error :   nhà thương 10\n",
            "{'tìm': 'UNKNOW', 'các': 'UNKNOW', 'nhà thương': 'OBJ', 'tại': 'OBJ_FEATURE', 'phở': 'OBJ_FEATURE', 'cuốn': 'OBJ_FEATURE', 'hương': 'LOCATION_STREET', 'mai': 'OBJ_FEATURE', 'vũ': 'pad', 'phạm': 'pad', 'hàm': 'LOCATION_PROVINCE'}\n",
            "{'UNKNOW': 'tìm các', 'OBJ': 'nhà thương', 'OBJ_FEATURE': 'tại phở cuốn mai', 'LOCATION_STREET': 'hương', 'pad': 'vũ phạm', 'LOCATION_PROVINCE': 'hàm'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJnTyEzHfolI",
        "outputId": "fe5e1b9b-ea48-42d7-d0e2-c576f90fbb78"
      },
      "source": [
        "# get word embedding\n",
        "from vncorenlp import VnCoreNLP\n",
        "print('Creating token word...')\n",
        "annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "print('token word created')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating token word...\n",
            "token word created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kssvB0jYrAef",
        "outputId": "a1aa47c9-e1d5-4991-c4ba-9ecfa4562af4"
      },
      "source": [
        "import requests\n",
        "#from flask import Flask, render_template, jsonify\n",
        "from flask import Flask, render_template, abort, request, jsonify, json,Response\n",
        "from flask import request, redirect, url_for\n",
        "import codecs\n",
        "import gensim\n",
        "from distutils.version import LooseVersion, StrictVersion\n",
        "import json\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "app.config['JSON_AS_ASCII'] = False\n",
        "\n",
        "global word2vec_model\n",
        "\n",
        "@app.route('/')\n",
        "def summary():\n",
        "  if request.method == \"GET\":\n",
        "    query = request.values['query'] or ''\n",
        "    query = str(query)\n",
        "    print( 'query = ' + query )\n",
        "    \n",
        "    # token word\n",
        "    try:\n",
        "      sen_words = annotator.tokenize(query)[0]\n",
        "      num_of_word = len(sen_words)\n",
        "    except:\n",
        "      print('Creating token word...')\n",
        "      annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "      print('token word created')\n",
        "      sen_words = annotator.tokenize(query)[0]\n",
        "      num_of_word = len(sen_words)\n",
        "    print(sen_words)\n",
        "        \n",
        "    # get word embedding\n",
        "    sen_words_embedd = get_word_embbeding(sen_words,max_length=42,embedd_dim = 300)\n",
        "    print(\"sen_words_embedd done!\")\n",
        "    # get char embedding\n",
        "    char_encode = get_char_encode(sen_words,42,32)\n",
        "\n",
        "    char_embedd = get_charEmbedd_form_encode(char_encode)\n",
        "    char_embedd = char_embedd.reshape(1,char_embedd.shape[0], char_embedd.shape[1],char_embedd.shape[2])\n",
        "    print(\"char_embedd done!\")\n",
        "    # predict\n",
        "    output = m.predict([char_embedd,sen_words_embedd])\n",
        "    print(\"output done!\")\n",
        "    # return [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5] or convert to encode tag\n",
        "    tag_encode = []\n",
        "    for i in range(42): \n",
        "      tag_encode.append(argmax(output[0,i,:]))\n",
        "\n",
        "    # return [street, homenumber,.....] or only get [num_of_word] first element=> convert to tag\n",
        "    tag_result=[]\n",
        "    for i in range(num_of_word):\n",
        "      tag_result.append(aphabet_tag[tag_encode[i]])\n",
        "      arr_tag_result=np.array(tag_result)\n",
        "\n",
        "    # return {'nhaf_hàng': OBJ,....} \n",
        "    result={}\n",
        "    for i in range(num_of_word):\n",
        "      result[sen_words[i]] = str(arr_tag_result[i])\n",
        "\n",
        "    print(result)\n",
        "    #==========================\n",
        "    dic_result = {}\n",
        "    for word in result:\n",
        "      tag = result[word]\n",
        "      try:\n",
        "        dic_result[tag] += \", \" + word\n",
        "      except:\n",
        "        dic_result[tag] = word\n",
        "    print(dic_result)\n",
        "\n",
        "    return jsonify(sen_words=sen_words, tags=tag_result, output=dic_result)\n",
        "if __name__ == \"__main__\":\n",
        "  import os\n",
        "  app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://96763f8082c6.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "query = tìm kiếm khách sạn quanh đây\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 01:31:02] \"\u001b[37mGET /?query=tìm%20kiếm%20khách%20sạn%20quanh%20đây HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'khách_sạn', 'quanh', 'đây']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'khách_sạn': 'OBJ', 'quanh': 'PRE', 'đây': 'LOCATION_SPECIAL'}\n",
            "{'UNKNOW': 'tìm_kiếm', 'OBJ': 'khách_sạn', 'PRE': 'quanh', 'LOCATION_SPECIAL': 'đây'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 01:31:02] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "query = tìm kiếm quán ăn gần 144 xuân thủy cầu giấy hà nội\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 01:56:29] \"\u001b[37mGET /?query=tìm%20kiếm%20quán%20ăn%20gần%20144%20xuân%20thủy%20cầu%20giấy%20hà%20nội HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'quán', 'ăn', 'gần', '144', 'xuân', 'thuỷ', 'cầu_giấy', 'hà_nội']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'quán': 'OBJ', 'ăn': 'OBJ', 'gần': 'PRE', '144': 'LOCATION_HOMENUMBER', 'xuân': 'LOCATION_WARD', 'thuỷ': 'LOCATION_WARD', 'cầu_giấy': 'LOCATION_DISTRICT', 'hà_nội': 'LOCATION_PROVINCE'}\n",
            "{'UNKNOW': 'tìm_kiếm', 'OBJ': 'quán, ăn', 'PRE': 'gần', 'LOCATION_HOMENUMBER': '144', 'LOCATION_WARD': 'xuân, thuỷ', 'LOCATION_DISTRICT': 'cầu_giấy', 'LOCATION_PROVINCE': 'hà_nội'}\n",
            "query = tìm kiếm quán ăn gần 144 xuân thủy cầu giấy\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 01:57:26] \"\u001b[37mGET /?query=tìm%20kiếm%20quán%20ăn%20gần%20144%20xuân%20thủy%20cầu%20giấy HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'quán', 'ăn', 'gần', '144', 'xuân', 'thuỷ', 'cầu_giấy']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'quán': 'OBJ', 'ăn': 'OBJ', 'gần': 'PRE', '144': 'LOCATION_HOMENUMBER', 'xuân': 'LOCATION_WARD', 'thuỷ': 'LOCATION_WARD', 'cầu_giấy': 'LOCATION_DISTRICT'}\n",
            "{'UNKNOW': 'tìm_kiếm', 'OBJ': 'quán, ăn', 'PRE': 'gần', 'LOCATION_HOMENUMBER': '144', 'LOCATION_WARD': 'xuân, thuỷ', 'LOCATION_DISTRICT': 'cầu_giấy'}\n",
            "query = tìm kiếm cửa hàng gần 144 xuân thủy\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 02:01:24] \"\u001b[37mGET /?query=tìm%20kiếm%20cửa%20hàng%20gần%20144%20xuân%20thủy HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'cửa_hàng', 'gần', '144', 'xuân', 'thuỷ']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'cửa_hàng': 'OBJ', 'gần': 'PRE', '144': 'pad', 'xuân': 'LOCATION_SPECIAL', 'thuỷ': 'LOCATION_SPECIAL'}\n",
            "{'UNKNOW': 'tìm_kiếm', 'OBJ': 'cửa_hàng', 'PRE': 'gần', 'pad': '144', 'LOCATION_SPECIAL': 'xuân, thuỷ'}\n",
            "query = tìm_kiếm các phòng khám tư nhân đang mở cửa ở quán bi a\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 02:03:48] \"\u001b[37mGET /?query=tìm_kiếm%20các%20phòng%20khám%20tư%20nhân%20đang%20mở%20cửa%20ở%20quán%20bi%20a HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm', '_', 'kiếm', 'các', 'phòng_khám_tư_nhân', 'đang', 'mở_cửa', 'ở', 'quán', 'bi_a']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm': 'UNKNOW', '_': 'OBJ', 'kiếm': 'OBJ', 'các': 'UNKNOW', 'phòng_khám_tư_nhân': 'OBJ', 'đang': 'OBJ_FEATURE', 'mở_cửa': 'OBJ_FEATURE', 'ở': 'PRE', 'quán': 'LOCATION_SPECIAL', 'bi_a': 'pad'}\n",
            "{'UNKNOW': 'tìm, các', 'OBJ': '_, kiếm, phòng_khám_tư_nhân', 'OBJ_FEATURE': 'đang, mở_cửa', 'PRE': 'ở', 'LOCATION_SPECIAL': 'quán', 'pad': 'bi_a'}\n",
            "query = tìm kiếm các phòng khám tư nhân đang mở cửa ở quán bi a\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 02:04:36] \"\u001b[37mGET /?query=tìm%20kiếm%20các%20phòng%20khám%20tư%20nhân%20đang%20mở%20cửa%20ở%20quán%20bi%20a HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'các', 'phòng_khám_tư_nhân', 'đang', 'mở_cửa', 'ở', 'quán', 'bi_a']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'các': 'UNKNOW', 'phòng_khám_tư_nhân': 'OBJ', 'đang': 'OBJ_FEATURE', 'mở_cửa': 'OBJ_FEATURE', 'ở': 'PRE', 'quán': 'LOCATION_SPECIAL', 'bi_a': 'pad'}\n",
            "{'UNKNOW': 'tìm_kiếm, các', 'OBJ': 'phòng_khám_tư_nhân', 'OBJ_FEATURE': 'đang, mở_cửa', 'PRE': 'ở', 'LOCATION_SPECIAL': 'quán', 'pad': 'bi_a'}\n",
            "query = tìm kiếm cáctrung tâm nghệ thuật đang mở cửa ở quán bi a\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 02:11:53] \"\u001b[37mGET /?query=tìm%20kiếm%20cáctrung%20tâm%20nghệ%20thuật%20đang%20mở%20cửa%20ở%20quán%20bi%20a HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'cáctrung', 'tâm', 'nghệ_thuật', 'đang', 'mở_cửa', 'ở', 'quán', 'bi_a']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'cáctrung': 'OBJ', 'tâm': 'OBJ', 'nghệ_thuật': 'OBJ', 'đang': 'OBJ_FEATURE', 'mở_cửa': 'OBJ_FEATURE', 'ở': 'PRE', 'quán': 'LOCATION_SPECIAL', 'bi_a': 'pad'}\n",
            "{'UNKNOW': 'tìm_kiếm', 'OBJ': 'cáctrung, tâm, nghệ_thuật', 'OBJ_FEATURE': 'đang, mở_cửa', 'PRE': 'ở', 'LOCATION_SPECIAL': 'quán', 'pad': 'bi_a'}\n",
            "query = tìm kiếm các trung tâm nghệ thuật đang mở cửa ở quán bi a\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 02:16:23] \"\u001b[37mGET /?query=tìm%20kiếm%20các%20trung%20tâm%20nghệ%20thuật%20đang%20mở%20cửa%20ở%20quán%20bi%20a HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'các', 'trung_tâm', 'nghệ_thuật', 'đang', 'mở_cửa', 'ở', 'quán', 'bi_a']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'các': 'UNKNOW', 'trung_tâm': 'OBJ', 'nghệ_thuật': 'OBJ', 'đang': 'OBJ_FEATURE', 'mở_cửa': 'OBJ_FEATURE', 'ở': 'PRE', 'quán': 'LOCATION_SPECIAL', 'bi_a': 'pad'}\n",
            "{'UNKNOW': 'tìm_kiếm, các', 'OBJ': 'trung_tâm, nghệ_thuật', 'OBJ_FEATURE': 'đang, mở_cửa', 'PRE': 'ở', 'LOCATION_SPECIAL': 'quán', 'pad': 'bi_a'}\n",
            "query = tìm kiếm các văn phòng giao hàng đang mở cửa ở quán bi a\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 02:18:09] \"\u001b[37mGET /?query=tìm%20kiếm%20các%20văn%20phòng%20giao%20hàng%20đang%20mở%20cửa%20ở%20quán%20bi%20a HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'các', 'văn_phòng_giao_hàng', 'đang', 'mở_cửa', 'ở', 'quán', 'bi_a']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'các': 'UNKNOW', 'văn_phòng_giao_hàng': 'OBJ', 'đang': 'OBJ_FEATURE', 'mở_cửa': 'OBJ_FEATURE', 'ở': 'PRE', 'quán': 'LOCATION_SPECIAL', 'bi_a': 'pad'}\n",
            "{'UNKNOW': 'tìm_kiếm, các', 'OBJ': 'văn_phòng_giao_hàng', 'OBJ_FEATURE': 'đang, mở_cửa', 'PRE': 'ở', 'LOCATION_SPECIAL': 'quán', 'pad': 'bi_a'}\n",
            "query = tìm kiếm các cửa hàng giày dép đang mở cửa ở quán bi a\n",
            "Creating token word...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Apr/2021 02:22:12] \"\u001b[37mGET /?query=tìm%20kiếm%20các%20cửa%20hàng%20giày%20dép%20đang%20mở%20cửa%20ở%20quán%20bi%20a HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token word created\n",
            "['tìm_kiếm', 'các', 'cửa_hàng_giày_dép', 'đang', 'mở_cửa', 'ở', 'quán', 'bi_a']\n",
            "sen_words_embedd done!\n",
            "char_embedd done!\n",
            "output done!\n",
            "{'tìm_kiếm': 'UNKNOW', 'các': 'UNKNOW', 'cửa_hàng_giày_dép': 'OBJ', 'đang': 'OBJ_FEATURE', 'mở_cửa': 'OBJ_FEATURE', 'ở': 'PRE', 'quán': 'LOCATION_SPECIAL', 'bi_a': 'pad'}\n",
            "{'UNKNOW': 'tìm_kiếm, các', 'OBJ': 'cửa_hàng_giày_dép', 'OBJ_FEATURE': 'đang, mở_cửa', 'PRE': 'ở', 'LOCATION_SPECIAL': 'quán', 'pad': 'bi_a'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}