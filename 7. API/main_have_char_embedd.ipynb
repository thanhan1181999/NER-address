{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_have_char_embedd.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgQllUjD0p87","executionInfo":{"status":"ok","timestamp":1619064372917,"user_tz":-420,"elapsed":65497,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"88185178-316b-452e-f767-52a2bdf69626"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iMPM2_La0ul8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064427369,"user_tz":-420,"elapsed":119938,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"6e3d86c2-7d2c-40e0-932c-a45221b6d017"},"source":["!pip install flask-ngrok\n","!pip install git+https://github.com/facebookresearch/fastText.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting flask-ngrok\n","  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n","Collecting git+https://github.com/facebookresearch/fastText.git\n","  Cloning https://github.com/facebookresearch/fastText.git to /tmp/pip-req-build-0_d0mf57\n","  Running command git clone -q https://github.com/facebookresearch/fastText.git /tmp/pip-req-build-0_d0mf57\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (54.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3086504 sha256=454b972492df7f644d7d9263c1991e91d309e2a9d7e8fd468da6659e116bff4b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-h1qkv2yh/wheels/69/f8/19/7f0ab407c078795bc9f86e1f6381349254f86fd7d229902355\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d48fmlv7ek7h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064610878,"user_tz":-420,"elapsed":88195,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"aa5ae91e-ac26-4905-daf1-13b7189587b1"},"source":["!pip install tensorflow==1.15\n","!pip install keras==2.2.5\n","!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 31kB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 30.7MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 43.9MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (54.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=02a05cf3eb0f2728ae102d3fe0a48cbc8375f6e74ae1ba7db4ff22f501c902b4\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, keras-applications, gast, tensorflow-estimator, tensorflow\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: tensorflow 2.4.1\n","    Uninstalling tensorflow-2.4.1:\n","      Successfully uninstalled tensorflow-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n","Collecting keras==2.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n","\u001b[K     |████████████████████████████████| 337kB 3.8MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.0.8)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.2.5\n","Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-sud64441\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-sud64441\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=631370b6ca0fe1d52827eb18088d97e7e6fabfef74b9259449e35567bf518ea3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-8c4f_5wf/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iYwlQRHnfMg1","executionInfo":{"status":"ok","timestamp":1619064610879,"user_tz":-420,"elapsed":82958,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}}},"source":["import numpy as np\n","from numpy import argmax"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7mFtI8qfSm4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064613466,"user_tz":-420,"elapsed":84625,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"a2093c00-7765-4279-d3d8-aaed9b507047"},"source":["from keras.models import Sequential, Model, Input\n","from keras.layers import LSTM, Dense, TimeDistributed, Activation, Bidirectional, Masking, Embedding, Dropout, Flatten, concatenate, Conv1D, MaxPool1D, Lambda, Softmax\n","from keras import backend as K\n","from keras_contrib.layers import CRF\n","from keras.utils import plot_model\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras_contrib.losses import  crf_loss\n","from keras_contrib.metrics import crf_accuracy"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"k-XKNXF5AMln","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064619519,"user_tz":-420,"elapsed":6043,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"c310a354-7058-40a0-b42b-712d4a3108d2"},"source":["from keras.models import load_model\n","m = load_model(\"/content/drive/MyDrive/model/model-1.h5\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OHnZKlbNYGSi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064709485,"user_tz":-420,"elapsed":96005,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"abe1e12e-5515-4216-db16-db5266c20934"},"source":["#get word embedding libary\n","import fasttext.util\n","print('Creating fastText...')\n","word2vec = fasttext.load_model('/content/drive/MyDrive/fast_text/cc.vi.300.bin')\n","print('fastText created!')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Creating fastText...\n","fastText created!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PKHABcXhSjC3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064716101,"user_tz":-420,"elapsed":102618,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"95d883e3-5108-4b53-ac99-f6396d4b653e"},"source":["!pip install vncorenlp"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting vncorenlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.7MB 2.5MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n","Building wheels for collected packages: vncorenlp\n","  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=6ccde0a69779e276c9b89937491e2b4c819c604b8dd763aaf8b39374924eeb18\n","  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n","Successfully built vncorenlp\n","Installing collected packages: vncorenlp\n","Successfully installed vncorenlp-1.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EeDy9kM_Sj53","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064723222,"user_tz":-420,"elapsed":109736,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"2a8b8744-fbd9-42ed-d5ba-ef9fa8bfcfdd"},"source":["# get word embedding\n","from vncorenlp import VnCoreNLP\n","print('Creating token word...')\n","annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","print('token word created')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Creating token word...\n","token word created\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D674NiI6Syo9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064723223,"user_tz":-420,"elapsed":109730,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"05e54ee4-f5f5-43d9-f32a-b5d1d8f8d6ba"},"source":["# aphabet tag library\n","tags = {'pad': 0, 'LOCATION_HOMENUMBER': 1, 'LOCATION_STREET': 2, 'LOCATION_WARD': 3, 'LOCATION_DISTRICT': 4, 'LOCATION_PROVINCE': 5, 'LOCATION_COUNTRY': 6, 'LOCATION_POSTCODE': 7, 'LOCATION_SPECIAL': 8, 'LOCATION_NER': 9, 'OBJ': 10, 'OBJ_FEATURE': 11, 'PRE': 12, 'UNKNOW': 13 }\n","aphabet_tag = []\n","for key in tags:\n","  aphabet_tag.append(key)\n","# aphabet_tag = ['pad', 'LOCATION_HOMENUMBER', 'LOCATION_STREET', 'LOCATION_WARD', 'LOCATION_DISTRICT', 'LOCATION_PROVINCE', 'LOCATION_COUNTRY', 'LOCATION_POSTCODE', 'OBJ', 'OBJ_FEATURE', 'PRE', 'UNKNOW', 'LOCATION_SPECIAL']\n","print(aphabet_tag)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["['pad', 'LOCATION_HOMENUMBER', 'LOCATION_STREET', 'LOCATION_WARD', 'LOCATION_DISTRICT', 'LOCATION_PROVINCE', 'LOCATION_COUNTRY', 'LOCATION_POSTCODE', 'LOCATION_SPECIAL', 'LOCATION_NER', 'OBJ', 'OBJ_FEATURE', 'PRE', 'UNKNOW']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AB5ojXXev9Dk","executionInfo":{"status":"ok","timestamp":1619064723224,"user_tz":-420,"elapsed":109728,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}}},"source":["def map_number_and_punct(word):\n","    # check hem va ngach\n","    num_of_seperate=0\n","    dem=0\n","    for char in word:\n","      if not char.isnumeric() and char!=\"/\":\n","        dem+=1        \n","      if char==\"/\":\n","        num_of_seperate+=1\n","    if dem==0:\n","      if len(word)>1 and num_of_seperate==1:\n","        return u'<ngach>'\n","      if len(word)>1 and num_of_seperate>2:\n","        return u'<hem>'\n","\n","    if re.match(r\"^[0-9]{2}0{3}$\", word):\n","        return u'<postcode>'\n","\n","    if word.isnumeric():\n","        word = u'<number>'\n","    elif word in [u',', u'<', u'.', u'>', u'/', u'?', u'..', u'...', u'....', u':', u';', u'\"', u\"'\", u'[', u'{', u']',\n","                  u'}', u'|', u'\\\\', u'`', u'~', u'!', u'@', u'#', u'$', u'%', u'^', u'&', u'*', u'(', u')', u'-', u'+',\n","                  u'=']:\n","        word = u'<punct>'\n","\n","    return word\n","\n","def get_word_embbeding(sen_words,max_length=32,embedd_dim = 300):\n","  X = np.empty([1, max_length, embedd_dim])\n","  unknown_embedd = np.random.uniform(-0.01, 0.01, [1, 300])\n","  length = len(sen_words)\n","  for i in range(length):\n","    try:\n","      vec = word2vec.get_word_vector(map_number_and_punct(sen_words[i])).tolist()\n","    except:\n","      vec = unknown_embedd\n","    X[0, i, :] = vec\n","  X[0,length:] = np.zeros([1, embedd_dim])\n","  return np.array(X)\n","# sen_words_embedd = get_word_embbeding(sen_words[0],max_length=25,embedd_dim = 300)\n","# print(sen_words_embedd.shape)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAVbVKV3ZpgG","executionInfo":{"status":"ok","timestamp":1619064723225,"user_tz":-420,"elapsed":109725,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}}},"source":["#get char embedding\n","def read_char_vocab(file_path):\n","  char_vocab = []\n","  for line in open(file_path, encoding='utf-8'):\n","    char_vocab.append(line.splitlines()[0])      \n","  return char_vocab\n","\n","def char_to_int(char_vocab_data):\n","  dic = {}\n","  index = -1\n","  for char in char_vocab_data:\n","    try:\n","      dic[char]\n","    except:\n","      index = index + 1\n","      dic[char]=index\n","  return dic\n"," \n","def get_char_encode(sentence,max_length_of_a_sentence,max_length_of_a_word):\n","  char_vocab_data = read_char_vocab(\"/content/drive/MyDrive/fast_text/VISCII_short.txt\")\n","  dic = char_to_int(char_vocab_data)\n","  # sentence  words is a sentence | ['i','am','an']\n","  sentence_encoded = np.zeros([max_length_of_a_sentence,max_length_of_a_word]) # 25*25\n","  for j in range(len(sentence)):  \n","    word = sentence[j].lower()\n","    # integer_encoded = [char_to_int[char] for char in word]\n","    word_encoded = np.zeros(max_length_of_a_word)\n","    for k in range(len(word)):\n","      char = word[k]\n","      try:\n","        word_encoded[k]= dic[char]\n","      except:\n","        print(\"error : \" + str(char)+\" \"+str(word)+\" \"+str(len(word)))\n","        word_encoded[k]= dic['[unk]']\n","    # sentence encoded\n","    sentence_encoded[j] = word_encoded\n","  return sentence_encoded\n","\n","def get_charEmbedd_form_encode(charEnocde):\n","  LEN_OF_VOCAB = 137\n","  shape = charEnocde.shape\n","  char_embedd = np.zeros([shape[0],shape[1],LEN_OF_VOCAB])\n","  for i in range(shape[0]):\n","    for j in range(shape[1]):\n","      char_int = charEnocde[i,j]\n","      char_int = char_int.astype(np.int64)\n","      onehot = np.zeros(LEN_OF_VOCAB)\n","      onehot[char_int] = 1\n","      char_embedd[i,j,:] = onehot\n","  return char_embedd\n","\n","# return onehot_encoded_of_a_sentence"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkdZwHxH362H","executionInfo":{"status":"ok","timestamp":1619064723831,"user_tz":-420,"elapsed":110323,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"e6f149d3-7b2b-44d5-a3be-8429038ee164"},"source":["!unzip /content/drive/MyDrive/ngrok/ngrok-stable-linux-amd64.zip"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/ngrok/ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BloS0Pnz4UA5","executionInfo":{"status":"ok","timestamp":1619064723832,"user_tz":-420,"elapsed":110317,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"d88e92a1-9934-4f81-8e8b-779b9141e646"},"source":["!./ngrok authtoken 1ow8g2aFZ4wILzg2dqHin5bUy5t_34f12x5zn89tGiLLCi4Wy"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6haWtvVihD-3","executionInfo":{"status":"ok","timestamp":1619064739734,"user_tz":-420,"elapsed":915,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"02ddba84-d8ea-4ed2-81bd-35c76399af38"},"source":["# query = \"số nhà 103 - A12\"\n","# # query = \"tìm các nơi tổ chức tang lễ đang mở cửa xung quanh khu này\"\n","# try:\n","#   sen_words = annotator.tokenize(query)[0]\n","#   num_of_word = len(sen_words)\n","# except:\n","#   print('Creating token word...')\n","#   annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","#   print('token word created')\n","#   sen_words = annotator.tokenize(query)[0]\n","#   num_of_word = len(sen_words)\n","# print(sen_words)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["['số', 'nhà', '103', '-', 'A12']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-opCmTfYVEG","executionInfo":{"status":"ok","timestamp":1619064880700,"user_tz":-420,"elapsed":746,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"f0990b03-d0ac-4687-c15a-68222c5d1a99"},"source":["# # query = \"tìm kiếm khách sạn 5 sao xung quanh chỗ này\"\n","# # query = \"sân bay đang mở cửa xung quanh 144 lô 14a trung hòa cầu giấy hà nội việt nam\"\n","# # query = \"tìm các nơi tổ chức tang lễ đang mở cửa xung quanh khu này\"\n","# query = \"14a ngõ 2 đức diễn kiều mai bắc từ liêm hà nội\"\n","# try:\n","#   sen_words = annotator.tokenize(query)[0]\n","#   num_of_word = len(sen_words)\n","# except:\n","#   print('Creating token word...')\n","#   annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","#   print('token word created')\n","#   sen_words = annotator.tokenize(query)[0]\n","#   num_of_word = len(sen_words)\n","# print(sen_words)\n","    \n","# # get word embedding\n","# sen_words_embedd = get_word_embbeding(sen_words,max_length=42,embedd_dim = 300)\n","    \n","# # get char embedding\n","# char_encode = get_char_encode(sen_words,42,32)\n","\n","# char_embedd = get_charEmbedd_form_encode(char_encode)\n","# char_embedd = char_embedd.reshape(1,char_embedd.shape[0], char_embedd.shape[1],char_embedd.shape[2])\n","\n","# # predict\n","# output = m.predict([char_embedd,sen_words_embedd])\n","\n","# # return [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5] or convert to encode tag\n","# tag_encode = []\n","# for i in range(42): \n","#   tag_encode.append(argmax(output[0,i,:]))\n","\n","# # return [street, homenumber,.....] or only get [num_of_word] first element=> convert to tag\n","# tag_result=[]\n","# for i in range(num_of_word):\n","#   tag_result.append(aphabet_tag[tag_encode[i]])\n","#   arr_tag_result=np.array(tag_result)\n","\n","# # return {'nhaf_hàng': OBJ,....} \n","# result={}\n","# for i in range(num_of_word):\n","#   result[sen_words[i]] = str(arr_tag_result[i])\n","\n","# print(result)\n","# #==========================\n","# dic_result = {}\n","# for word in result:\n","#   tag = result[word]\n","#   try:\n","#     dic_result[tag] += \" \" + word\n","#   except:\n","#     dic_result[tag] = word\n","# print(dic_result)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["['14a', 'ngõ', '2', 'đức', 'diễn_kiều', 'mai', 'bắc', 'từ_liêm', 'hà_nội']\n","{'14a': 'LOCATION_HOMENUMBER', 'ngõ': 'LOCATION_STREET', '2': 'LOCATION_STREET', 'đức': 'LOCATION_STREET', 'diễn_kiều': 'LOCATION_STREET', 'mai': 'LOCATION_WARD', 'bắc': 'LOCATION_WARD', 'từ_liêm': 'LOCATION_PROVINCE', 'hà_nội': 'LOCATION_PROVINCE'}\n","{'LOCATION_HOMENUMBER': '14a', 'LOCATION_STREET': 'ngõ 2 đức diễn_kiều', 'LOCATION_WARD': 'mai bắc', 'LOCATION_PROVINCE': 'từ_liêm hà_nội'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kssvB0jYrAef","outputId":"0884731e-cf95-4660-a0a8-9ffd6c315878"},"source":["import requests\n","#from flask import Flask, render_template, jsonify\n","from flask import Flask, render_template, abort, request, jsonify, json,Response\n","from flask import request, redirect, url_for\n","import codecs\n","import gensim\n","from distutils.version import LooseVersion, StrictVersion\n","import json\n","from flask_ngrok import run_with_ngrok\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)\n","app.config['JSON_AS_ASCII'] = False\n","\n","global word2vec_model\n","\n","@app.route('/')\n","def summary():\n","  if request.method == \"GET\":\n","    query = request.values['query'] or ''\n","    query = str(query)\n","    print( 'query = ' + query )\n","    \n","    # token word\n","    try:\n","      sen_words = annotator.tokenize(query)[0]\n","      num_of_word = len(sen_words)\n","    except:\n","      print('Creating token word...')\n","      annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","      print('token word created')\n","      sen_words = annotator.tokenize(query)[0]\n","      num_of_word = len(sen_words)\n","    print(sen_words)\n","        \n","    # get word embedding\n","    sen_words_embedd = get_word_embbeding(sen_words,max_length=42,embedd_dim = 300)\n","    print(\"sen_words_embedd done!\")\n","    # get char embedding\n","    char_encode = get_char_encode(sen_words,42,32)\n","\n","    char_embedd = get_charEmbedd_form_encode(char_encode)\n","    char_embedd = char_embedd.reshape(1,char_embedd.shape[0], char_embedd.shape[1],char_embedd.shape[2])\n","    print(\"char_embedd done!\")\n","    # predict\n","    output = m.predict([char_embedd,sen_words_embedd])\n","    print(\"output done!\")\n","    # return [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5] or convert to encode tag\n","    tag_encode = []\n","    for i in range(42): \n","      tag_encode.append(argmax(output[0,i,:]))\n","\n","    # return [street, homenumber,.....] or only get [num_of_word] first element=> convert to tag\n","    tag_result=[]\n","    for i in range(num_of_word):\n","      tag_result.append(aphabet_tag[tag_encode[i]])\n","      arr_tag_result=np.array(tag_result)\n","\n","    # return {'nhaf_hàng': OBJ,....} \n","    result={}\n","    for i in range(num_of_word):\n","      result[sen_words[i]] = str(arr_tag_result[i])\n","\n","    print(result)\n","    #==========================\n","    dic_result = {}\n","    for word in result:\n","      tag = result[word]\n","      try:\n","        dic_result[tag] += \", \" + word\n","      except:\n","        dic_result[tag] = word\n","    print(dic_result)\n","\n","    return jsonify(sen_words=sen_words, tags=tag_result, output=dic_result)\n","if __name__ == \"__main__\":\n","  import os\n","  app.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"],"name":"stdout"},{"output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"],"name":"stderr"},{"output_type":"stream","text":[" * Running on http://f7362b018b58.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n","query = \"144 xuân thủy cầu giấy hà nội\"\n","Creating token word...\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [22/Apr/2021 04:16:35] \"\u001b[37mGET /?query=%22144%20xuân%20thủy%20cầu%20giấy%20hà%20nội%22 HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["token word created\n","['\"', '144', 'xuân', 'thuỷ', 'cầu_giấy', 'hà_nội', '\"']\n","sen_words_embedd done!\n","char_embedd done!\n","output done!\n","{'\"': 'pad', '144': 'LOCATION_STREET', 'xuân': 'LOCATION_WARD', 'thuỷ': 'LOCATION_WARD', 'cầu_giấy': 'LOCATION_DISTRICT', 'hà_nội': 'LOCATION_PROVINCE'}\n","{'pad': '\"', 'LOCATION_STREET': '144', 'LOCATION_WARD': 'xuân, thuỷ', 'LOCATION_DISTRICT': 'cầu_giấy', 'LOCATION_PROVINCE': 'hà_nội'}\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [22/Apr/2021 04:16:36] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"],"name":"stderr"},{"output_type":"stream","text":["query = 144 xuân thủy cầu giấy hà nội\n","Creating token word...\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [22/Apr/2021 04:21:30] \"\u001b[37mGET /?query=144%20xuân%20thủy%20cầu%20giấy%20hà%20nội HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["token word created\n","['144', 'xuân', 'thuỷ', 'cầu_giấy', 'hà_nội']\n","sen_words_embedd done!\n","char_embedd done!\n","output done!\n","{'144': 'LOCATION_HOMENUMBER', 'xuân': 'LOCATION_WARD', 'thuỷ': 'LOCATION_WARD', 'cầu_giấy': 'LOCATION_DISTRICT', 'hà_nội': 'LOCATION_PROVINCE'}\n","{'LOCATION_HOMENUMBER': '144', 'LOCATION_WARD': 'xuân, thuỷ', 'LOCATION_DISTRICT': 'cầu_giấy', 'LOCATION_PROVINCE': 'hà_nội'}\n"],"name":"stdout"}]}]}