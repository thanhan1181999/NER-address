{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_word_embedd_all_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxbgQLqx0mOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18ad8cd-1b0a-4a18-9af0-5de738950960"
      },
      "source": [
        "!pip install git+https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/fastText.git\n",
            "  Cloning https://github.com/facebookresearch/fastText.git to /tmp/pip-req-build-ca9l40t_\n",
            "  Running command git clone -q https://github.com/facebookresearch/fastText.git /tmp/pip-req-build-ca9l40t_\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (56.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3086315 sha256=9487aefa09634eb84879aff8fffaee5cb30cac2854eecc14410708275c4c04f6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f4k6i2qn/wheels/69/f8/19/7f0ab407c078795bc9f86e1f6381349254f86fd7d229902355\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xeb4ZlmI1HFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb5c650-b359-4fce-90c1-f26c3c728e59"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHrYj0nH019e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce1ee37-f0ef-4779-9132-974a5333da63"
      },
      "source": [
        "#get word embedding libary\n",
        "import fasttext.util\n",
        "print('Creating fastText...')\n",
        "word2vec = fasttext.load_model('/content/drive/MyDrive/fast_text/cc.vi.300.bin')\n",
        "print('fastText created!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating fastText...\n",
            "fastText created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoZ3ZK6pFwr9"
      },
      "source": [
        "import re\n",
        "def map_number_and_punct(word):\n",
        "    # check hem va ngach\n",
        "    num_of_seperate=0\n",
        "    dem=0\n",
        "    for char in word:\n",
        "      if not char.isnumeric() and char!=\"/\":\n",
        "        dem+=1        \n",
        "      if char==\"/\":\n",
        "        num_of_seperate+=1\n",
        "    if dem==0:\n",
        "      if len(word)>1 and num_of_seperate==1:\n",
        "        return u'<ngach>'\n",
        "      if len(word)>1 and num_of_seperate>1:\n",
        "        return u'<hem>'\n",
        "\n",
        "    # if re.match(r\"^[0-9]{2}0{3}$\", word):\n",
        "    #     return u'<postcode>'\n",
        "    \n",
        "    if word.isnumeric():\n",
        "        word = u'<number>'\n",
        "    elif word in [u',', u'<', u'.', u'>', u'/', u'?', u'..', u'...', u'....', u':', u';', u'\"', u\"'\", u'[', u'{', u']',\n",
        "                  u'}', u'|', u'\\\\', u'`', u'~', u'!', u'@', u'#', u'$', u'%', u'^', u'&', u'*', u'(', u')', u'-', u'+',\n",
        "                  u'=']:\n",
        "        word = u'<punct>'\n",
        "\n",
        "    # word = word.replace(\"_\",\" \")\n",
        "    \n",
        "    return word"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVJr7L4o2XdL"
      },
      "source": [
        "import numpy as np\n",
        "import codecs\n",
        "def read_data(input_file):\n",
        "    with codecs.open(input_file, 'r', 'utf-8') as f:\n",
        "        word_list = [] \n",
        "        words = []\n",
        "        for line in f:\n",
        "            line = line.split()\n",
        "            if len(line) > 0:\n",
        "                words.append( map_number_and_punct(line[0].lower()) )\n",
        "            else:\n",
        "                word_list.append(words)\n",
        "                words = []\n",
        "    return word_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Ui5njT03Lc"
      },
      "source": [
        "def construct_tensor_word(word_sentences, unknown_embedd, embedd_dim, max_length,ouput_file):\n",
        "    fileout = open(ouput_file, 'a+', encoding='utf8')\n",
        "    X = np.empty([1, max_length, embedd_dim])\n",
        "    for i in range(len(word_sentences)):\n",
        "        words = word_sentences[i] # words is a sentence\n",
        "        # print(\"=====sentence \"+str(i)+\" ==========\")\n",
        "        length = len(words)\n",
        "        # run through all word in a sentence\n",
        "        for j in range(length):\n",
        "            word = words[j].lower()\n",
        "            while True:\n",
        "                try:\n",
        "                    embedd = word2vec.get_word_vector(query).tolist()\n",
        "                    embedd = np.ones([1, embedd_dim])\n",
        "                    break\n",
        "                except:\n",
        "                    embedd = unknown_embedd[0]\n",
        "                    break         \n",
        "            X[0, j, :] = embedd\n",
        "\n",
        "        X[0, length:] = np.zeros([1, embedd_dim])\n",
        "        # print(X.shape)\n",
        "\n",
        "        np.savetxt(fileout, X[0])\n",
        "    fileout.close()\n",
        "    return X"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPG0VNx4zwTS",
        "outputId": "ad4f23be-d577-4b95-b0cc-eb76fc94d13a"
      },
      "source": [
        "import math\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "link_folder = \"/content/drive/MyDrive/fast_text/split data/data_0\"\n",
        "index = 0\n",
        "while path.exists(link_folder):\n",
        "  print(\"DATA {}\".format(index))\n",
        "  \n",
        "  link_train = \"/content/drive/MyDrive/fast_text/split data/data_{}/train/data_no_tag.txt\".format(index)\n",
        "  link_val = \"/content/drive/MyDrive/fast_text/split data/data_{}/val/data_no_tag.txt\".format(index)\n",
        "  link_test = \"/content/drive/MyDrive/fast_text/split data/data_{}/test/data_no_tag_pred.txt\".format(index)\n",
        "  out_train = \"/content/drive/MyDrive/fast_text/split data/data_{}/train/word_embedd.txt\".format(index)\n",
        "  out_val = \"/content/drive/MyDrive/fast_text/split data/data_{}/val/word_embedd.txt\".format(index)\n",
        "  out_test = \"/content/drive/MyDrive/fast_text/split data/data_{}/test/word_embedd.txt\".format(index)\n",
        "\n",
        "  val_word_sentences = read_data(link_val)\n",
        "  train_word_sentences = read_data(link_train)\n",
        "  test_word_sentences = read_data(link_test)\n",
        "\n",
        "  unknown_embedd = np.random.uniform(-0.01, 0.01, [1, 300])\n",
        "  word_train = construct_tensor_word(train_word_sentences, unknown_embedd, 300, 42,out_train)\n",
        "  word_val = construct_tensor_word(val_word_sentences, unknown_embedd, 300, 42,out_val)\n",
        "  word_test = construct_tensor_word(test_word_sentences, unknown_embedd, 300, 42, out_test)\n",
        "\n",
        "  index+=1\n",
        "  link_folder = \"/content/drive/MyDrive/fast_text/split data/data_{}\".format(index)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA 0\n",
            "DATA 1\n",
            "DATA 2\n",
            "DATA 3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}