{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_model-with-test-data",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HcFJQGJHsRk",
        "outputId": "bee54226-29e7-46ed-d487-a460f41cbc55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D9G9KinUMl4",
        "outputId": "67860757-4387-4daa-9013-9e08d1db125e"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install keras==2.2.5\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 22kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 27.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=d6ab2c754832ab3f5f0d485cc4eb0589fc3138d26ffdf750fc109a6b8fd0a756\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.5\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-5d7xad3t\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-5d7xad3t\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=e9ca37b9183b121f0abee9e46f78ed4b1dec548f3268d2ec7568a71ca4f8bc43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8y5djtey/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4cn6ojhJoIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01694ef7-a29a-4e5f-c2a7-de0f0d02973e"
      },
      "source": [
        "from keras.models import Sequential, Model, Input\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Activation, Bidirectional, Masking, Embedding, Dropout, Flatten, concatenate, Conv1D, MaxPool1D, Lambda, Softmax\n",
        "from keras import backend as K\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras_contrib.losses import  crf_loss\n",
        "from keras_contrib.metrics import crf_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4yZNJ0BIuye",
        "outputId": "62884ab9-7236-4e97-f8c2-6c994eccd91a"
      },
      "source": [
        "from keras.models import load_model\n",
        "m = load_model(\"/content/drive/MyDrive/model/model-use-rdr-4500-1-1-1.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kg12gPnX48w"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import argmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgDDEXrX5lKa"
      },
      "source": [
        "import re\n",
        "def map_number_and_punct(word):\n",
        "    # check hem va ngach\n",
        "    num_of_seperate=0\n",
        "    dem=0\n",
        "    for char in word:\n",
        "      if not char.isnumeric() and char!=\"/\":\n",
        "        dem+=1        \n",
        "      if char==\"/\":\n",
        "        num_of_seperate+=1\n",
        "    if dem==0:\n",
        "      if len(word)>1 and num_of_seperate==1:\n",
        "        return u'<ngach>'\n",
        "      if len(word)>1 and num_of_seperate>1:\n",
        "        return u'<hem>'\n",
        "\n",
        "    # if re.match(r\"^[0-9]{2}0{3}$\", word):\n",
        "    #     return u'<postcode>'\n",
        "    \n",
        "    if word.isnumeric():\n",
        "        word = u'<number>'\n",
        "    elif word in [u',', u'<', u'.', u'>', u'/', u'?', u'..', u'...', u'....', u':', u';', u'\"', u\"'\", u'[', u'{', u']',\n",
        "                  u'}', u'|', u'\\\\', u'`', u'~', u'!', u'@', u'#', u'$', u'%', u'^', u'&', u'*', u'(', u')', u'-', u'+',\n",
        "                  u'=']:\n",
        "        word = u'<punct>'\n",
        "\n",
        "    # word = word.replace(\"_\",\" \")\n",
        "    \n",
        "    return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq7ixX3f5c7L"
      },
      "source": [
        "import numpy as np\n",
        "import codecs\n",
        "def read_data(input_file):\n",
        "    with codecs.open(input_file, 'r', 'utf-8') as f:\n",
        "        word_list = [] \n",
        "        words = []\n",
        "        for line in f:\n",
        "            line = line.split()\n",
        "            if len(line) > 0:\n",
        "                words.append( map_number_and_punct(line[0].lower()[0:31]) )\n",
        "            else:\n",
        "                word_list.append(words)\n",
        "                words = []\n",
        "    return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Sc4IuA5rgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b8d797-2b64-45c4-afb0-4c0b4cb306f8"
      },
      "source": [
        "test_word_sentences = read_data('/content/drive/MyDrive/fast_text/test/data_no_tag_pre.txt')\n",
        "print(len(test_word_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJDBnDFBJVxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c052ef0-af65-4d9f-c996-1632f9e41d1e"
      },
      "source": [
        "#get char embedding\n",
        "def read_char_vocab(file_path):\n",
        "  char_vocab = []\n",
        "  for line in open(file_path, encoding='utf-8'):\n",
        "    char_vocab.append(line.splitlines()[0])      \n",
        "  return char_vocab\n",
        "\n",
        "def char_to_int(char_vocab_data):\n",
        "  dic = {}\n",
        "  index = -1\n",
        "  for char in char_vocab_data:\n",
        "    try:\n",
        "      dic[char]\n",
        "    except:\n",
        "      index = index + 1\n",
        "      dic[char]=index\n",
        "  return dic\n",
        " \n",
        "def get_char_encode(sentence,max_length_of_a_sentence,max_length_of_a_word):\n",
        "  char_vocab_data = read_char_vocab(\"/content/drive/MyDrive/fast_text/VISCII_short.txt\")\n",
        "  dic = char_to_int(char_vocab_data)\n",
        "  # sentence  words is a sentence | ['i','am','an']\n",
        "  sentence_encoded = np.zeros([max_length_of_a_sentence,max_length_of_a_word]) # 25*25\n",
        "  for j in range(len(sentence)):  \n",
        "    word = sentence[j].lower()\n",
        "    # integer_encoded = [char_to_int[char] for char in word]\n",
        "    word_encoded = np.zeros(max_length_of_a_word)\n",
        "    for k in range(len(word)):\n",
        "      char = word[k]\n",
        "      try:\n",
        "        word_encoded[k]= dic[char]\n",
        "      except:\n",
        "        # print(\"error : \" + str(char)+\" \"+str(word)+\" \"+str(len(word)))\n",
        "        word_encoded[k]= dic['[unk]']\n",
        "    # sentence encoded\n",
        "    sentence_encoded[j] = word_encoded\n",
        "  return sentence_encoded\n",
        "\n",
        "def get_charEmbedd_form_encode(charEnocde):\n",
        "  LEN_OF_VOCAB = 137\n",
        "  shape = charEnocde.shape\n",
        "  char_embedd = np.zeros([shape[0],shape[1],LEN_OF_VOCAB])\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      char_int = charEnocde[i,j]\n",
        "      char_int = char_int.astype(np.int64)\n",
        "      onehot = np.zeros(LEN_OF_VOCAB)\n",
        "      onehot[char_int] = 1\n",
        "      char_embedd[i,j,:] = onehot\n",
        "  return char_embedd\n",
        "\n",
        "char_embedd = []\n",
        "for sen_token in test_word_sentences:\n",
        "  char_en = get_char_encode(sen_token,42,32)\n",
        "  char_em = get_charEmbedd_form_encode(char_en)\n",
        "  char_embedd.append(char_em)\n",
        "\n",
        "char_embedd = np.array(char_embedd)\n",
        "print(char_embedd.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 32, 137)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDcYdy7JbvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834df405-7587-4d0f-c8a5-76a385d7b66b"
      },
      "source": [
        "tag = np.loadtxt('/content/drive/MyDrive/fast_text/test/tag_embedd.txt')\n",
        "tag = tag.reshape(int(tag.shape[0]/42), 42, tag.shape[1])\n",
        "print(tag.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWsDe0wRJdqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f0d119-669c-45fd-d53a-b93e0d45faff"
      },
      "source": [
        "word = np.loadtxt('/content/drive/MyDrive/fast_text/test/word_embedd.txt')\n",
        "word = word.reshape(int(word.shape[0]/42), 42, word.shape[1])\n",
        "print(word.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ2UMExMJUeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4a8572-8bf9-4138-fc2a-54175e905097"
      },
      "source": [
        "y_pred = m.predict([char_embedd, word])\n",
        "y_true = tag\n",
        "print(y_pred.shape)\n",
        "print(y_true.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 13)\n",
            "(900, 42, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8Mxu-n6MKwf"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SefHoZhuMNSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddb7b8e-a05d-4132-89d5-f07fd311bd59"
      },
      "source": [
        "y_pred = y_pred.reshape(y_pred.shape[0]*y_pred.shape[1], y_pred.shape[2])\n",
        "print(y_pred.shape)\n",
        "y_pred = np.argmax(y_pred,axis=1)\n",
        "print(y_pred.shape)\n",
        "y_pred = y_pred.reshape(int(y_pred.shape[0]/42), 42)\n",
        "print(y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37800, 13)\n",
            "(37800,)\n",
            "(900, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgJDJblRMN5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a30d99-5e5b-4cd4-f521-793fa6b85cc6"
      },
      "source": [
        "y_true = y_true.reshape(y_true.shape[0]*y_true.shape[1], y_true.shape[2])\n",
        "print(y_true.shape)\n",
        "y_true = np.argmax(y_true,axis=1)\n",
        "print(y_true.shape)\n",
        "y_true = y_true.reshape(int(y_true.shape[0]/42), 42)\n",
        "print(y_true.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37800, 13)\n",
            "(37800,)\n",
            "(900, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUIjfzaxZtd7"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLRqx-0hF1Qk"
      },
      "source": [
        "def arr_tag_in_a_tag_sen(sen_pred, sen_true, tag):\n",
        "  j=0\n",
        "  bat_dau=-1\n",
        "  ket_thuc=-1\n",
        "  while j < len(sen_pred):\n",
        "    if sen_true[j]==tag:\n",
        "      bat_dau=j\n",
        "      ket_thuc=j\n",
        "      while sen_true[ket_thuc+1]==1:\n",
        "        ket_thuc=ket_thuc+1\n",
        "      ket_thuc=ket_thuc+1\n",
        "      break\n",
        "    else:\n",
        "        j=j+1\n",
        "\n",
        "  if bat_dau==-1 and ket_thuc==-1:\n",
        "    return 0\n",
        "  \n",
        "  a = sen_pred[bat_dau:ket_thuc]\n",
        "  b = sen_true[bat_dau:ket_thuc]\n",
        "  a = np.array(a)\n",
        "  b = np.array(b)\n",
        "  if (a==b).all():\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6kgFk60F3vy"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc=0\n",
        "false=0\n",
        "\n",
        "homenumber_true = 0\n",
        "street_true = 0 \n",
        "ward_true = 0\n",
        "district_true = 0\n",
        "province_true = 0\n",
        "country_true = 0\n",
        "postcode_true = 0\n",
        "ner_true = 0\n",
        "obj_true = 0\n",
        "obj_feature_true = 0\n",
        "pre_true = 0\n",
        "\n",
        "homenumber_false = 0\n",
        "street_false = 0\n",
        "ward_false = 0\n",
        "district_false = 0\n",
        "province_false = 0\n",
        "country_false = 0\n",
        "postcode_false = 0\n",
        "ner_false = 0\n",
        "obj_false = 0\n",
        "obj_feature_false = 0\n",
        "pre_false = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbIAZz0JF573"
      },
      "source": [
        "for i in range(len(y_true)):\n",
        "  sen_pred = y_pred[i]\n",
        "  sen_true = y_true[i]\n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 1)==1:\n",
        "    homenumber_true+=1\n",
        "  else:\n",
        "    homenumber_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 1)==1:\n",
        "    homenumber_true+=1\n",
        "  else:\n",
        "    homenumber_false+=1\n",
        "\n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 2)==1:\n",
        "    street_true+=1\n",
        "  else:\n",
        "    street_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 3)==1:\n",
        "    ward_true+=1\n",
        "  else:\n",
        "    ward_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 4)==1:\n",
        "    district_true+=1\n",
        "  else:\n",
        "    district_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 5)==1:\n",
        "    province_true+=1\n",
        "  else:\n",
        "    province_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 6)==1:\n",
        "    country_true+=1\n",
        "  else:\n",
        "    country_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 7)==1:\n",
        "    postcode_true+=1\n",
        "  else:\n",
        "    postcode_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 8)==1:\n",
        "    ner_true+=1\n",
        "  else:\n",
        "    ner_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 9)==1:\n",
        "    obj_true+=1\n",
        "  else:\n",
        "    obj_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 10)==1:\n",
        "    obj_feature_true+=1\n",
        "  else:\n",
        "    obj_feature_false+=1\n",
        "\n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 11)==1:\n",
        "    pre_true+=1\n",
        "  else:\n",
        "    pre_false+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q71CvkdUMZrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ab42c0-8327-453b-fb91-32167965ebc7"
      },
      "source": [
        "acc = homenumber_true+ street_true+ ward_true+ district_true+ province_true+ country_true+ postcode_true+ ner_true+ obj_true+ obj_feature_true+ pre_true\n",
        "false = homenumber_false+ street_false+ ward_false+ district_false+ province_false+ country_false+ postcode_false+ ner_false+ obj_false+ obj_feature_false+ pre_false\n",
        "print(acc)\n",
        "print(false)\n",
        "print('accuracy = {}'.format(acc/(acc+false)) )\n",
        "ti_le_dung = format(acc/(acc+false)*100, '.2f')\n",
        "print(ti_le_dung)\n",
        "\n",
        "print(\"homenumber : {} true, {} false\".format(homenumber_true, homenumber_false))\n",
        "print(\"street : {} true, {} false\".format(street_true, street_false))\n",
        "print(\"ward : {} true, {} false\".format(ward_true, ward_false))\n",
        "print(\"district : {} true, {} false\".format(district_true, district_false))\n",
        "print(\"province : {} true, {} false\".format(province_true, province_false))\n",
        "print(\"country : {} true, {} false\".format(country_true, country_false))\n",
        "print(\"postcode : {} true, {} false\".format(postcode_true, postcode_false))\n",
        "print(\"ner : {} true, {} false\".format(ner_true, ner_false))\n",
        "print(\"obj : {} true, {} false\".format(obj_true, obj_false))\n",
        "print(\"obj_feature : {} true, {} false\".format(obj_feature_true, obj_feature_false))\n",
        "print(\"pre : {} true, {} false\".format(pre_true, pre_false))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5710\n",
            "5090\n",
            "accuracy = 0.5287037037037037\n",
            "52.87\n",
            "homenumber : 984 true, 816 false\n",
            "street : 587 true, 313 false\n",
            "ward : 273 true, 627 false\n",
            "district : 584 true, 316 false\n",
            "province : 584 true, 316 false\n",
            "country : 307 true, 593 false\n",
            "postcode : 584 true, 316 false\n",
            "ner : 293 true, 607 false\n",
            "obj : 595 true, 305 false\n",
            "obj_feature : 327 true, 573 false\n",
            "pre : 592 true, 308 false\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dRzissoMddN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78cba58b-a955-49dd-d1f3-29926a9e8d39"
      },
      "source": [
        "dict = []\n",
        "y_true = y_true.reshape(y_true.shape[0]*y_true.shape[1])\n",
        "y_pred = y_pred.reshape(y_pred.shape[0]*y_pred.shape[1])\n",
        "for element in y_true:\n",
        "  if element not in dict:\n",
        "    dict.append(element)\n",
        "# print(dict)\n",
        "dict.sort()\n",
        "print(dict)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_true, y_pred, labels=dict)\n",
        "for i in range(len(cnf_matrix)):\n",
        "  for j in range(len(cnf_matrix)):\n",
        "    cnf_matrix[i][j] = round(cnf_matrix[i][j], 3)\n",
        "print(cnf_matrix)\n",
        "\n",
        "arr_to_cal_predict = []\n",
        "for i in range(len(cnf_matrix)):\n",
        "  tong= 0\n",
        "  for j in range(len(cnf_matrix)):\n",
        "    tong+=cnf_matrix[j][i]\n",
        "  arr_to_cal_predict.append(tong)\n",
        "print(arr_to_cal_predict)\n",
        "# lấy độ predict\n",
        "predict = []\n",
        "for i in range(len(cnf_matrix)):\n",
        "  tong=cnf_matrix[i][i]/arr_to_cal_predict[i]\n",
        "  predict.append(tong)\n",
        "\n",
        "print(\"Độ đo P của từng tag: \")\n",
        "print(predict)\n",
        "print(len(predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "[[27027     0     0     0     0     0     0     0     8     0     0     0\n",
            "     33]\n",
            " [    0   638    18     0     0     0     0     0     0     0     0    11\n",
            "      0]\n",
            " [    0     4  2178    20    19     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    0     0     0   273     4     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    0     0     0     0   584    16     0     0     0     0     0     0\n",
            "      0]\n",
            " [    0     0     0     0     0   584     6    10     0     0     0     0\n",
            "      0]\n",
            " [    0     0     0     0     0     0   307     6     0     0     0     0\n",
            "      0]\n",
            " [   15     0     0     0     0     0     0   584     0     0     0     0\n",
            "      1]\n",
            " [    6     0     0     0     1     0     0     4  1001     4     0     5\n",
            "      3]\n",
            " [    0     0     1     0     0     0     0     0     3  1128    25     8\n",
            "      5]\n",
            " [    0     4     4     0     0     0     0     2    11    20   663    10\n",
            "     13]\n",
            " [    0     1     3     0     0     0     0     0     1     0     0   958\n",
            "      1]\n",
            " [    1     0     0     0     0     0     0     0     3     2     0     0\n",
            "   1563]]\n",
            "[27049, 647, 2204, 293, 608, 600, 313, 606, 1027, 1154, 688, 992, 1619]\n",
            "Độ đo P của từng tag: \n",
            "[0.9991866612444082, 0.9860896445131375, 0.9882032667876588, 0.931740614334471, 0.9605263157894737, 0.9733333333333334, 0.9808306709265175, 0.9636963696369637, 0.9746835443037974, 0.9774696707105719, 0.9636627906976745, 0.9657258064516129, 0.9654107473749228]\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uitCUG6Mhl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a040a2bb-c860-4df8-cc40-167d18c04d83"
      },
      "source": [
        "normalized_confusion_matrix = cnf_matrix/cnf_matrix.sum(axis = 1, keepdims = True)\n",
        "for i in range(len(normalized_confusion_matrix)):\n",
        "  for j in range(len(normalized_confusion_matrix)):\n",
        "    normalized_confusion_matrix[i][j] = round(normalized_confusion_matrix[i][j], 3)\n",
        "print(normalized_confusion_matrix)\n",
        "\n",
        "# lấy độ recall\n",
        "recall = []\n",
        "for i in range(len(normalized_confusion_matrix)):\n",
        "  recall.append(normalized_confusion_matrix[i][i])\n",
        "\n",
        "print(\"Độ đo R của từng tag: \")\n",
        "print(recall)\n",
        "print(len(recall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.998 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.001]\n",
            " [0.    0.957 0.027 0.    0.    0.    0.    0.    0.    0.    0.    0.016\n",
            "  0.   ]\n",
            " [0.    0.002 0.981 0.009 0.009 0.    0.    0.    0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.986 0.014 0.    0.    0.    0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.    0.973 0.027 0.    0.    0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.    0.    0.973 0.01  0.017 0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.    0.    0.    0.981 0.019 0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.025 0.    0.    0.    0.    0.    0.    0.973 0.    0.    0.    0.\n",
            "  0.002]\n",
            " [0.006 0.    0.    0.    0.001 0.    0.    0.004 0.978 0.004 0.    0.005\n",
            "  0.003]\n",
            " [0.    0.    0.001 0.    0.    0.    0.    0.    0.003 0.964 0.021 0.007\n",
            "  0.004]\n",
            " [0.    0.006 0.006 0.    0.    0.    0.    0.003 0.015 0.028 0.912 0.014\n",
            "  0.018]\n",
            " [0.    0.001 0.003 0.    0.    0.    0.    0.    0.001 0.    0.    0.994\n",
            "  0.001]\n",
            " [0.001 0.    0.    0.    0.    0.    0.    0.    0.002 0.001 0.    0.\n",
            "  0.996]]\n",
            "Độ đo R của từng tag: \n",
            "[0.998, 0.957, 0.981, 0.986, 0.973, 0.973, 0.981, 0.973, 0.978, 0.964, 0.912, 0.994, 0.996]\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHUzSoVKulgw",
        "outputId": "edf9bd20-d789-4900-ef13-decd8127a1c7"
      },
      "source": [
        "# lấy độ F1\n",
        "f1 = []\n",
        "for i in range(len(recall)):\n",
        "  p = predict[i]\n",
        "  r = recall[i]\n",
        "  f1.append(2*p*r/(p+r))\n",
        "\n",
        "print(\"Độ đo F1 của từng tag: \")\n",
        "print(f1)\n",
        "print(len(f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Độ đo F1 của từng tag: \n",
            "[0.9985929780850736, 0.9713270743466124, 0.9845884587629293, 0.9581027161513299, 0.9667229223943162, 0.9731666381229662, 0.980915328155713, 0.9683258381204426, 0.9763389558024658, 0.9706881099204805, 0.9371199018021537, 0.9796589384624248, 0.9804668457867113]\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}