{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_model-with-test-data",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HcFJQGJHsRk",
        "outputId": "972cafc1-05e1-42b9-ad0e-b1a1bdd3d02b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D9G9KinUMl4",
        "outputId": "c7b00fe9-760e-4a36-df12-bc7dc276bf38"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install keras==2.2.5\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: keras==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-38t9kbp2\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-38t9kbp2\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=3f6ccd29e45f563b1971e8b1711d09e8f3544474f696d0ae1f4c7c163b4d1ac8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sha58wra/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4cn6ojhJoIp"
      },
      "source": [
        "from keras.models import Sequential, Model, Input\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Activation, Bidirectional, Masking, Embedding, Dropout, Flatten, concatenate, Conv1D, MaxPool1D, Lambda, Softmax\n",
        "from keras import backend as K\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras_contrib.losses import  crf_loss\n",
        "from keras_contrib.metrics import crf_accuracy"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4yZNJ0BIuye"
      },
      "source": [
        "from keras.models import load_model\n",
        "m = load_model(\"/content/drive/MyDrive/model/model-use-rdr-4500-1-1-1-ver2.h5\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kg12gPnX48w"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import argmax"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgDDEXrX5lKa"
      },
      "source": [
        "import re\n",
        "def map_number_and_punct(word):\n",
        "    # check hem va ngach\n",
        "    num_of_seperate=0\n",
        "    dem=0\n",
        "    for char in word:\n",
        "      if not char.isnumeric() and char!=\"/\":\n",
        "        dem+=1        \n",
        "      if char==\"/\":\n",
        "        num_of_seperate+=1\n",
        "    if dem==0:\n",
        "      if len(word)>1 and num_of_seperate==1:\n",
        "        return u'<ngach>'\n",
        "      if len(word)>1 and num_of_seperate>1:\n",
        "        return u'<hem>'\n",
        "\n",
        "    # if re.match(r\"^[0-9]{2}0{3}$\", word):\n",
        "    #     return u'<postcode>'\n",
        "    \n",
        "    if word.isnumeric():\n",
        "        word = u'<number>'\n",
        "    elif word in [u',', u'<', u'.', u'>', u'/', u'?', u'..', u'...', u'....', u':', u';', u'\"', u\"'\", u'[', u'{', u']',\n",
        "                  u'}', u'|', u'\\\\', u'`', u'~', u'!', u'@', u'#', u'$', u'%', u'^', u'&', u'*', u'(', u')', u'-', u'+',\n",
        "                  u'=']:\n",
        "        word = u'<punct>'\n",
        "\n",
        "    # word = word.replace(\"_\",\" \")\n",
        "    \n",
        "    return word"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq7ixX3f5c7L"
      },
      "source": [
        "import numpy as np\n",
        "import codecs\n",
        "def read_data(input_file):\n",
        "    with codecs.open(input_file, 'r', 'utf-8') as f:\n",
        "        word_list = [] \n",
        "        words = []\n",
        "        for line in f:\n",
        "            line = line.split()\n",
        "            if len(line) > 0:\n",
        "                words.append( map_number_and_punct(line[0].lower()[0:31]) )\n",
        "            else:\n",
        "                word_list.append(words)\n",
        "                words = []\n",
        "    return word_list"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Sc4IuA5rgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54bb07ca-d0c0-4680-a501-d1d4a23927da"
      },
      "source": [
        "test_word_sentences = read_data('/content/drive/MyDrive/fast_text/test/data_no_tag_pred.txt')\n",
        "print(len(test_word_sentences))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJDBnDFBJVxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f37e01-fb40-4606-bc44-a8d78ccf6b84"
      },
      "source": [
        "#get char embedding\n",
        "def read_char_vocab(file_path):\n",
        "  char_vocab = []\n",
        "  for line in open(file_path, encoding='utf-8'):\n",
        "    char_vocab.append(line.splitlines()[0])      \n",
        "  return char_vocab\n",
        "\n",
        "def char_to_int(char_vocab_data):\n",
        "  dic = {}\n",
        "  index = -1\n",
        "  for char in char_vocab_data:\n",
        "    try:\n",
        "      dic[char]\n",
        "    except:\n",
        "      index = index + 1\n",
        "      dic[char]=index\n",
        "  return dic\n",
        " \n",
        "def get_char_encode(sentence,max_length_of_a_sentence,max_length_of_a_word):\n",
        "  char_vocab_data = read_char_vocab(\"/content/drive/MyDrive/fast_text/VISCII_short.txt\")\n",
        "  dic = char_to_int(char_vocab_data)\n",
        "  # sentence  words is a sentence | ['i','am','an']\n",
        "  sentence_encoded = np.zeros([max_length_of_a_sentence,max_length_of_a_word]) # 25*25\n",
        "  for j in range(len(sentence)):  \n",
        "    word = sentence[j].lower()\n",
        "    # integer_encoded = [char_to_int[char] for char in word]\n",
        "    word_encoded = np.zeros(max_length_of_a_word)\n",
        "    for k in range(len(word)):\n",
        "      char = word[k]\n",
        "      try:\n",
        "        word_encoded[k]= dic[char]\n",
        "      except:\n",
        "        # print(\"error : \" + str(char)+\" \"+str(word)+\" \"+str(len(word)))\n",
        "        word_encoded[k]= dic['[unk]']\n",
        "    # sentence encoded\n",
        "    sentence_encoded[j] = word_encoded\n",
        "  return sentence_encoded\n",
        "\n",
        "def get_charEmbedd_form_encode(charEnocde):\n",
        "  LEN_OF_VOCAB = 137\n",
        "  shape = charEnocde.shape\n",
        "  char_embedd = np.zeros([shape[0],shape[1],LEN_OF_VOCAB])\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      char_int = charEnocde[i,j]\n",
        "      char_int = char_int.astype(np.int64)\n",
        "      onehot = np.zeros(LEN_OF_VOCAB)\n",
        "      onehot[char_int] = 1\n",
        "      char_embedd[i,j,:] = onehot\n",
        "  return char_embedd\n",
        "\n",
        "char_embedd = []\n",
        "for sen_token in test_word_sentences:\n",
        "  char_en = get_char_encode(sen_token,42,32)\n",
        "  char_em = get_charEmbedd_form_encode(char_en)\n",
        "  char_embedd.append(char_em)\n",
        "\n",
        "char_embedd = np.array(char_embedd)\n",
        "print(char_embedd.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 32, 137)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDcYdy7JbvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0825e99d-e29a-43d3-a1dd-4a8e89b3f94b"
      },
      "source": [
        "tag = np.loadtxt('/content/drive/MyDrive/fast_text/test/tag_embedd.txt')\n",
        "tag = tag.reshape(int(tag.shape[0]/42), 42, tag.shape[1])\n",
        "print(tag.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWsDe0wRJdqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c80e1b9-ae35-466b-a9d4-c9c9e40797a1"
      },
      "source": [
        "word = np.loadtxt('/content/drive/MyDrive/fast_text/test/word_embedd.txt')\n",
        "word = word.reshape(int(word.shape[0]/42), 42, word.shape[1])\n",
        "print(word.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ2UMExMJUeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e64124-f586-478d-8a3e-5ee388320db1"
      },
      "source": [
        "y_pred = m.predict([char_embedd, word])\n",
        "y_true = tag\n",
        "print(y_pred.shape)\n",
        "print(y_true.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 42, 13)\n",
            "(900, 42, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8Mxu-n6MKwf"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SefHoZhuMNSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20bc286-a840-46a3-dcee-e40f9919dcbb"
      },
      "source": [
        "y_pred = y_pred.reshape(y_pred.shape[0]*y_pred.shape[1], y_pred.shape[2])\n",
        "print(y_pred.shape)\n",
        "y_pred = np.argmax(y_pred,axis=1)\n",
        "print(y_pred.shape)\n",
        "y_pred = y_pred.reshape(int(y_pred.shape[0]/42), 42)\n",
        "print(y_pred.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37800, 13)\n",
            "(37800,)\n",
            "(900, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgJDJblRMN5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87431899-da9f-4574-99b0-5e8677911d86"
      },
      "source": [
        "y_true = y_true.reshape(y_true.shape[0]*y_true.shape[1], y_true.shape[2])\n",
        "print(y_true.shape)\n",
        "y_true = np.argmax(y_true,axis=1)\n",
        "print(y_true.shape)\n",
        "y_true = y_true.reshape(int(y_true.shape[0]/42), 42)\n",
        "print(y_true.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37800, 13)\n",
            "(37800,)\n",
            "(900, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUIjfzaxZtd7"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLRqx-0hF1Qk"
      },
      "source": [
        "def arr_tag_in_a_tag_sen(sen_pred, sen_true, tag):\n",
        "  j=0\n",
        "  bat_dau=-1\n",
        "  ket_thuc=-1\n",
        "  while j < len(sen_pred):\n",
        "    if sen_true[j]==tag:\n",
        "      bat_dau=j\n",
        "      ket_thuc=j\n",
        "      while sen_true[ket_thuc+1]==1:\n",
        "        ket_thuc=ket_thuc+1\n",
        "      ket_thuc=ket_thuc+1\n",
        "      break\n",
        "    else:\n",
        "        j=j+1\n",
        "\n",
        "  if bat_dau==-1 and ket_thuc==-1:\n",
        "    return 0\n",
        "  \n",
        "  a = sen_pred[bat_dau:ket_thuc]\n",
        "  b = sen_true[bat_dau:ket_thuc]\n",
        "  a = np.array(a)\n",
        "  b = np.array(b)\n",
        "  if (a==b).all():\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6kgFk60F3vy"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc=0\n",
        "false=0\n",
        "\n",
        "homenumber_true = 0\n",
        "street_true = 0 \n",
        "ward_true = 0\n",
        "district_true = 0\n",
        "province_true = 0\n",
        "country_true = 0\n",
        "postcode_true = 0\n",
        "ner_true = 0\n",
        "obj_true = 0\n",
        "obj_feature_true = 0\n",
        "pre_true = 0\n",
        "\n",
        "homenumber_false = 0\n",
        "street_false = 0\n",
        "ward_false = 0\n",
        "district_false = 0\n",
        "province_false = 0\n",
        "country_false = 0\n",
        "postcode_false = 0\n",
        "ner_false = 0\n",
        "obj_false = 0\n",
        "obj_feature_false = 0\n",
        "pre_false = 0"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbIAZz0JF573"
      },
      "source": [
        "for i in range(len(y_true)):\n",
        "  sen_pred = y_pred[i]\n",
        "  sen_true = y_true[i]\n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 1)==1:\n",
        "    homenumber_true+=1\n",
        "  else:\n",
        "    homenumber_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 1)==1:\n",
        "    homenumber_true+=1\n",
        "  else:\n",
        "    homenumber_false+=1\n",
        "\n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 2)==1:\n",
        "    street_true+=1\n",
        "  else:\n",
        "    street_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 3)==1:\n",
        "    ward_true+=1\n",
        "  else:\n",
        "    ward_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 4)==1:\n",
        "    district_true+=1\n",
        "  else:\n",
        "    district_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 5)==1:\n",
        "    province_true+=1\n",
        "  else:\n",
        "    province_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 6)==1:\n",
        "    country_true+=1\n",
        "  else:\n",
        "    country_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 7)==1:\n",
        "    postcode_true+=1\n",
        "  else:\n",
        "    postcode_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 8)==1:\n",
        "    ner_true+=1\n",
        "  else:\n",
        "    ner_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 9)==1:\n",
        "    obj_true+=1\n",
        "  else:\n",
        "    obj_false+=1\n",
        "  \n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 10)==1:\n",
        "    obj_feature_true+=1\n",
        "  else:\n",
        "    obj_feature_false+=1\n",
        "\n",
        "  if arr_tag_in_a_tag_sen(sen_pred, sen_true, 11)==1:\n",
        "    pre_true+=1\n",
        "  else:\n",
        "    pre_false+=1"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q71CvkdUMZrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96513da4-6dad-4d8e-92d3-1dc4028cd6eb"
      },
      "source": [
        "acc = homenumber_true+ street_true+ ward_true+ district_true+ province_true+ country_true+ postcode_true+ ner_true+ obj_true+ obj_feature_true+ pre_true\n",
        "false = homenumber_false+ street_false+ ward_false+ district_false+ province_false+ country_false+ postcode_false+ ner_false+ obj_false+ obj_feature_false+ pre_false\n",
        "print(acc)\n",
        "print(false)\n",
        "print('accuracy = {}'.format(acc/(acc+false)) )\n",
        "ti_le_dung = format(acc/(acc+false)*100, '.2f')\n",
        "print(ti_le_dung)\n",
        "\n",
        "print(\"homenumber : {} true, {} false\".format(homenumber_true, homenumber_false))\n",
        "print(\"street : {} true, {} false\".format(street_true, street_false))\n",
        "print(\"ward : {} true, {} false\".format(ward_true, ward_false))\n",
        "print(\"district : {} true, {} false\".format(district_true, district_false))\n",
        "print(\"province : {} true, {} false\".format(province_true, province_false))\n",
        "print(\"country : {} true, {} false\".format(country_true, country_false))\n",
        "print(\"postcode : {} true, {} false\".format(postcode_true, postcode_false))\n",
        "print(\"ner : {} true, {} false\".format(ner_true, ner_false))\n",
        "print(\"obj : {} true, {} false\".format(obj_true, obj_false))\n",
        "print(\"obj_feature : {} true, {} false\".format(obj_feature_true, obj_feature_false))\n",
        "print(\"pre : {} true, {} false\".format(pre_true, pre_false))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5677\n",
            "5123\n",
            "accuracy = 0.5256481481481482\n",
            "52.56\n",
            "homenumber : 974 true, 826 false\n",
            "street : 587 true, 313 false\n",
            "ward : 308 true, 592 false\n",
            "district : 585 true, 315 false\n",
            "province : 585 true, 315 false\n",
            "country : 302 true, 598 false\n",
            "postcode : 585 true, 315 false\n",
            "ner : 287 true, 613 false\n",
            "obj : 589 true, 311 false\n",
            "obj_feature : 316 true, 584 false\n",
            "pre : 559 true, 341 false\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dRzissoMddN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb12e9a-4eb5-4301-ea21-7abaa1ec914b"
      },
      "source": [
        "dict = []\n",
        "y_true = y_true.reshape(y_true.shape[0]*y_true.shape[1])\n",
        "y_pred = y_pred.reshape(y_pred.shape[0]*y_pred.shape[1])\n",
        "for element in y_true:\n",
        "  if element not in dict:\n",
        "    dict.append(element)\n",
        "# print(dict)\n",
        "dict.sort()\n",
        "print(dict)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_true, y_pred, labels=dict)\n",
        "for i in range(len(cnf_matrix)):\n",
        "  for j in range(len(cnf_matrix)):\n",
        "    cnf_matrix[i][j] = round(cnf_matrix[i][j], 3)\n",
        "print(cnf_matrix)\n",
        "\n",
        "arr_to_cal_predict = []\n",
        "for i in range(len(cnf_matrix)):\n",
        "  tong= 0\n",
        "  for j in range(len(cnf_matrix)):\n",
        "    tong+=cnf_matrix[j][i]\n",
        "  arr_to_cal_predict.append(tong)\n",
        "print(arr_to_cal_predict)\n",
        "# lấy độ predict\n",
        "predict = []\n",
        "for i in range(len(cnf_matrix)):\n",
        "  tong=cnf_matrix[i][i]/arr_to_cal_predict[i]\n",
        "  predict.append(tong)\n",
        "\n",
        "print(\"Độ đo P của từng tag: \")\n",
        "print(predict)\n",
        "print(len(predict))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "[[26997     0     0     0     0     0     0     0     6     0     0     0\n",
            "     43]\n",
            " [    0   635    18     0     0     0     0     0     0     0     0     6\n",
            "      0]\n",
            " [    0     7  2176    14    14     0     0     0     0     1     0     0\n",
            "      0]\n",
            " [    0     0     0   308     3     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    0     0     0     0   585    15     0     0     0     0     0     0\n",
            "      0]\n",
            " [    0     0     0     0     0   585     1    14     0     0     0     0\n",
            "      0]\n",
            " [    0     0     0     0     0     0   302     1     0     0     0     0\n",
            "      0]\n",
            " [   14     0     0     0     0     0     0   585     0     0     0     0\n",
            "      1]\n",
            " [    6     0     1     0     0     0     0     4   971     5     1    10\n",
            "      8]\n",
            " [    0     6     5     0     0     0     0     0    16  1120    32     5\n",
            "      6]\n",
            " [    0     0     1     0     0     0     0     0    14    43   659    10\n",
            "      5]\n",
            " [    0     0     6     0     0     0     0     0     6     0    13   942\n",
            "     14]\n",
            " [    3     0     1     0     0     0     0     0     3     3     2    18\n",
            "   1530]]\n",
            "[27020, 648, 2208, 322, 602, 600, 303, 604, 1016, 1172, 707, 991, 1607]\n",
            "Độ đo P của từng tag: \n",
            "[0.9991487786824574, 0.9799382716049383, 0.9855072463768116, 0.9565217391304348, 0.9717607973421927, 0.975, 0.9966996699669967, 0.9685430463576159, 0.9557086614173228, 0.9556313993174061, 0.9321074964639321, 0.9505549949545913, 0.9520846297448662]\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uitCUG6Mhl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7147a1-65f5-47ca-847b-8def451889bb"
      },
      "source": [
        "normalized_confusion_matrix = cnf_matrix/cnf_matrix.sum(axis = 1, keepdims = True)\n",
        "for i in range(len(normalized_confusion_matrix)):\n",
        "  for j in range(len(normalized_confusion_matrix)):\n",
        "    normalized_confusion_matrix[i][j] = round(normalized_confusion_matrix[i][j], 3)\n",
        "print(normalized_confusion_matrix)\n",
        "\n",
        "# lấy độ recall\n",
        "recall = []\n",
        "for i in range(len(normalized_confusion_matrix)):\n",
        "  recall.append(normalized_confusion_matrix[i][i])\n",
        "\n",
        "print(\"Độ đo R của từng tag: \")\n",
        "print(recall)\n",
        "print(len(recall))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.998 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.002]\n",
            " [0.    0.964 0.027 0.    0.    0.    0.    0.    0.    0.    0.    0.009\n",
            "  0.   ]\n",
            " [0.    0.003 0.984 0.006 0.006 0.    0.    0.    0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.99  0.01  0.    0.    0.    0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.    0.975 0.025 0.    0.    0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.    0.    0.975 0.002 0.023 0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.    0.    0.    0.    0.    0.    0.997 0.003 0.    0.    0.    0.\n",
            "  0.   ]\n",
            " [0.023 0.    0.    0.    0.    0.    0.    0.975 0.    0.    0.    0.\n",
            "  0.002]\n",
            " [0.006 0.    0.001 0.    0.    0.    0.    0.004 0.965 0.005 0.001 0.01\n",
            "  0.008]\n",
            " [0.    0.005 0.004 0.    0.    0.    0.    0.    0.013 0.941 0.027 0.004\n",
            "  0.005]\n",
            " [0.    0.    0.001 0.    0.    0.    0.    0.    0.019 0.059 0.9   0.014\n",
            "  0.007]\n",
            " [0.    0.    0.006 0.    0.    0.    0.    0.    0.006 0.    0.013 0.96\n",
            "  0.014]\n",
            " [0.002 0.    0.001 0.    0.    0.    0.    0.    0.002 0.002 0.001 0.012\n",
            "  0.981]]\n",
            "Độ đo R của từng tag: \n",
            "[0.998, 0.964, 0.984, 0.99, 0.975, 0.975, 0.997, 0.975, 0.965, 0.941, 0.9, 0.96, 0.981]\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHUzSoVKulgw",
        "outputId": "b6bd5d25-ff69-42bc-ae6b-39796ffce724"
      },
      "source": [
        "# lấy độ F1\n",
        "f1 = []\n",
        "for i in range(len(recall)):\n",
        "  p = predict[i]\n",
        "  r = recall[i]\n",
        "  f1.append(2*p*r/(p+r))\n",
        "\n",
        "print(\"Độ đo F1 của từng tag: \")\n",
        "print(f1)\n",
        "print(len(f1))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Độ đo F1 của từng tag: \n",
            "[0.9985740589471, 0.9719037971789481, 0.9847530464472833, 0.972972972972973, 0.9733777038269551, 0.975, 0.9968498123627069, 0.9717607973421927, 0.9603318575001024, 0.9482592633247737, 0.9157724079363854, 0.9552541513499809, 0.9663260546466452]\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}